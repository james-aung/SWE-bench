{"repo": "psf/requests", "pull_number": 6716, "instance_id": "psf__requests-6716", "issue_numbers": ["6715"], "base_commit": "88dce9d854797c05d0ff296b70e0430535ef8aaf", "patch": "diff --git a/HISTORY.md b/HISTORY.md\nindex 4fca5894d7..327a4072e6 100644\n--- a/HISTORY.md\n+++ b/HISTORY.md\n@@ -6,6 +6,13 @@ dev\n \n - \\[Short description of non-trivial change.\\]\n \n+2.32.3 (2024-05-24)\n+-------------------\n+\n+**Bugfixes**\n+- Fix bug breaking the ability to specify custom SSLContexts in sub-classes of\n+  HTTPAdapter. (#6716)\n+\n 2.32.2 (2024-05-21)\n -------------------\n \ndiff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex 42fabe527c..4766693f56 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -83,16 +83,20 @@ def _urllib3_request_context(\n     request: \"PreparedRequest\",\n     verify: \"bool | str | None\",\n     client_cert: \"typing.Tuple[str, str] | str | None\",\n+    poolmanager: \"PoolManager\",\n ) -> \"(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])\":\n     host_params = {}\n     pool_kwargs = {}\n     parsed_request_url = urlparse(request.url)\n     scheme = parsed_request_url.scheme.lower()\n     port = parsed_request_url.port\n+    poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n+    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n+\n     cert_reqs = \"CERT_REQUIRED\"\n     if verify is False:\n         cert_reqs = \"CERT_NONE\"\n-    elif verify is True:\n+    elif verify is True and not has_poolmanager_ssl_context:\n         pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n     elif isinstance(verify, str):\n         if not os.path.isdir(verify):\n@@ -375,23 +379,83 @@ def build_response(self, req, resp):\n \n         return response\n \n+    def build_connection_pool_key_attributes(self, request, verify, cert=None):\n+        \"\"\"Build the PoolKey attributes used by urllib3 to return a connection.\n+\n+        This looks at the PreparedRequest, the user-specified verify value,\n+        and the value of the cert parameter to determine what PoolKey values\n+        to use to select a connection from a given urllib3 Connection Pool.\n+\n+        The SSL related pool key arguments are not consistently set. As of\n+        this writing, use the following to determine what keys may be in that\n+        dictionary:\n+\n+        * If ``verify`` is ``True``, ``\"ssl_context\"`` will be set and will be the\n+          default Requests SSL Context\n+        * If ``verify`` is ``False``, ``\"ssl_context\"`` will not be set but\n+          ``\"cert_reqs\"`` will be set\n+        * If ``verify`` is a string, (i.e., it is a user-specified trust bundle)\n+          ``\"ca_certs\"`` will be set if the string is not a directory recognized\n+          by :py:func:`os.path.isdir`, otherwise ``\"ca_certs_dir\"`` will be\n+          set.\n+        * If ``\"cert\"`` is specified, ``\"cert_file\"`` will always be set. If\n+          ``\"cert\"`` is a tuple with a second item, ``\"key_file\"`` will also\n+          be present\n+\n+        To override these settings, one may subclass this class, call this\n+        method and use the above logic to change parameters as desired. For\n+        example, if one wishes to use a custom :py:class:`ssl.SSLContext` one\n+        must both set ``\"ssl_context\"`` and based on what else they require,\n+        alter the other keys to ensure the desired behaviour.\n+\n+        :param request:\n+            The PreparedReqest being sent over the connection.\n+        :type request:\n+            :class:`~requests.models.PreparedRequest`\n+        :param verify:\n+            Either a boolean, in which case it controls whether\n+            we verify the server's TLS certificate, or a string, in which case it\n+            must be a path to a CA bundle to use.\n+        :param cert:\n+            (optional) Any user-provided SSL certificate for client\n+            authentication (a.k.a., mTLS). This may be a string (i.e., just\n+            the path to a file which holds both certificate and key) or a\n+            tuple of length 2 with the certificate file path and key file\n+            path.\n+        :returns:\n+            A tuple of two dictionaries. The first is the \"host parameters\"\n+            portion of the Pool Key including scheme, hostname, and port. The\n+            second is a dictionary of SSLContext related parameters.\n+        \"\"\"\n+        return _urllib3_request_context(request, verify, cert, self.poolmanager)\n+\n     def get_connection_with_tls_context(self, request, verify, proxies=None, cert=None):\n         \"\"\"Returns a urllib3 connection for the given request and TLS settings.\n         This should not be called from user code, and is only exposed for use\n         when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n \n-        :param request: The :class:`PreparedRequest <PreparedRequest>` object\n-            to be sent over the connection.\n-        :param verify: Either a boolean, in which case it controls whether\n-            we verify the server's TLS certificate, or a string, in which case it\n-            must be a path to a CA bundle to use.\n-        :param proxies: (optional) The proxies dictionary to apply to the request.\n-        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n-        :rtype: urllib3.ConnectionPool\n+        :param request:\n+            The :class:`PreparedRequest <PreparedRequest>` object to be sent\n+            over the connection.\n+        :param verify:\n+            Either a boolean, in which case it controls whether we verify the\n+            server's TLS certificate, or a string, in which case it must be a\n+            path to a CA bundle to use.\n+        :param proxies:\n+            (optional) The proxies dictionary to apply to the request.\n+        :param cert:\n+            (optional) Any user-provided SSL certificate to be used for client\n+            authentication (a.k.a., mTLS).\n+        :rtype:\n+            urllib3.ConnectionPool\n         \"\"\"\n         proxy = select_proxy(request.url, proxies)\n         try:\n-            host_params, pool_kwargs = _urllib3_request_context(request, verify, cert)\n+            host_params, pool_kwargs = self.build_connection_pool_key_attributes(\n+                request,\n+                verify,\n+                cert,\n+            )\n         except ValueError as e:\n             raise InvalidURL(e, request=request)\n         if proxy:\n", "test_patch": "", "problem_statement": "SSLV3_ALERT_HANDSHAKE_FAILURE after upgrade from 2.31.0 to 2.32.2\nAfter upgrading to requests 2.32.* our custom SSL adapter doesn't seem to working anymore. This is how it looks like\r\n\r\n```\r\nclass CustomSSLAdapter(HTTPAdapter):\r\n    def __init__(self):\r\n        self.context = ssl.create_default_context()\r\n\r\n        certfile = ...\r\n        keyfile = ...\r\n        password = ...\r\n        self.context.load_cert_chain(certfile=certfile, keyfile=keyfile, password=password)\r\n        \r\n        super().__init__()\r\n\r\n    def init_poolmanager(self, *args, **kwargs):\r\n        kwargs['ssl_context'] = self.context\r\n        return super().init_poolmanager(*args, **kwargs)\r\n```\r\n\r\nWe have tried to mount this only for the target url but also for \"http://\". The code was working find with version 2.31.0.\r\n\r\n## Expected Result\r\n\r\nThe adapter should still work.\r\n\r\n## Actual Result\r\n\r\nWe got the following error:\r\n\r\n```\r\nSSLError: HTTPSConnectionPool(host=..., port=543): Max retries exceeded with url: ... (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:1007)')))\r\n```\r\n\n", "hints_text": "This is running on databricks using python 3.10.12. \n@nateprewitt this is a different side-effect of #6655. I think to enable this particular use case we need to expose a way for sub-classes to muck with the pool kwargs, something like\r\n\r\n\r\n```py\r\nclass HTTPAdapter(BaseAdapter):\r\n    def build_pool_kwargs(self, request, verify, cert=None):\r\n        return _urllib3_request_context(request, verify, cert)\r\n```\r\n\r\nThat way a subclass like this could do something like:\r\n\r\n\r\n```py\r\nclass CustomSSLAdapter(HTTPAdapter):\r\n    def build_pool_kwargs(self, request, verify, cert=None):\r\n        params, kwargs = super().build_pool_kwargs(request, verify, cert)\r\n        params[\"ssl_context\"] = self.context\r\n```\r\n\r\nThoughts? I'm not particularly fond of this but it seems like the least intrusive way to preserve the fix for the CVE while giving people the ability to use custom SSLContexts the way they already were", "created_at": "2024-05-22T11:53:49Z"}
{"repo": "psf/requests", "pull_number": 6644, "instance_id": "psf__requests-6644", "issue_numbers": ["6643"], "base_commit": "382fc2c0c6c0ef0874bc65bc1175f97c073e5086", "patch": "diff --git a/src/requests/adapters.py b/src/requests/adapters.py\nindex eb240fa954..fc5606bdcb 100644\n--- a/src/requests/adapters.py\n+++ b/src/requests/adapters.py\n@@ -390,6 +390,9 @@ def request_url(self, request, proxies):\n             using_socks_proxy = proxy_scheme.startswith(\"socks\")\n \n         url = request.path_url\n+        if url.startswith(\"//\"):  # Don't confuse urllib3\n+            url = f\"/{url.lstrip('/')}\"\n+\n         if is_proxied_http_request and not using_socks_proxy:\n             url = urldefragauth(request.url)\n \n", "test_patch": "diff --git a/tests/test_adapters.py b/tests/test_adapters.py\nnew file mode 100644\nindex 0000000000..6c55d5a130\n--- /dev/null\n+++ b/tests/test_adapters.py\n@@ -0,0 +1,8 @@\n+import requests.adapters\n+\n+\n+def test_request_url_trims_leading_path_separators():\n+    \"\"\"See also https://github.com/psf/requests/issues/6643.\"\"\"\n+    a = requests.adapters.HTTPAdapter()\n+    p = requests.Request(method=\"GET\", url=\"http://127.0.0.1:10000//v:h\").prepare()\n+    assert \"/v:h\" == a.request_url(p, {})\n", "problem_statement": "Leading slash in uri followed by column fails\nLeading slash in uri followed by column fails.\r\n\r\n## Expected Result\r\n\r\n```python\r\nrequests.get('http://127.0.0.1:10000//v:h')\r\n<Response [200]>\r\n```\r\n\r\n## Actual Result\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/url.py\", line 425, in parse_url\r\n    host, port = _HOST_PORT_RE.match(host_port).groups()  # type: ignore[union-attr]\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'groups'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.11/site-packages/requests/api.py\", line 73, in get\r\n    return request(\"get\", url, params=params, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n           ^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 711, in urlopen\r\n    parsed_url = parse_url(url)\r\n                 ^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/url.py\", line 451, in parse_url\r\n    raise LocationParseError(source_url) from e\r\nurllib3.exceptions.LocationParseError: Failed to parse: //v:h\r\n```\r\n\r\n## Reproduction Steps\r\n\r\n```python\r\nimport requests\r\nrequests.get('http://127.0.0.1:10000//v:h')\r\n\r\n```\r\n\r\n## System Information\r\n\r\n    $ python -m requests.help\r\n\r\n```json\r\n{\r\n  \"chardet\": {\r\n    \"version\": null\r\n  },\r\n  \"charset_normalizer\": {\r\n    \"version\": \"3.3.2\"\r\n  },\r\n  \"cryptography\": {\r\n    \"version\": \"\"\r\n  },\r\n  \"idna\": {\r\n    \"version\": \"3.6\"\r\n  },\r\n  \"implementation\": {\r\n    \"name\": \"CPython\",\r\n    \"version\": \"3.11.8\"\r\n  },\r\n  \"platform\": {\r\n    \"release\": \"5.10.209-198.812.amzn2.x86_64\",\r\n    \"system\": \"Linux\"\r\n  },\r\n  \"pyOpenSSL\": {\r\n    \"openssl_version\": \"\",\r\n    \"version\": null\r\n  },\r\n  \"requests\": {\r\n    \"version\": \"2.31.0\"\r\n  },\r\n  \"system_ssl\": {\r\n    \"version\": \"300000b0\"\r\n  },\r\n  \"urllib3\": {\r\n    \"version\": \"2.2.1\"\r\n  },\r\n  \"using_charset_normalizer\": true,\r\n  \"using_pyopenssl\": false\r\n}\r\n```\r\n\r\n<!-- This command is only available on Requests v2.16.4 and greater. Otherwise,\r\nplease provide some basic information about your system (Python version,\r\noperating system, &c). -->\r\n\n", "hints_text": "I can reproduce this with\r\n\r\n```py\r\nimport urllib3\r\n\r\nurllib3.PoolManager().urlopen(method=\"GET\", url=\"http://127.0.0.1:10000//v:h\")\r\n```\r\n\r\ncc @sethmlarson @pquentin \nAh I see the problem, both the PoolManager and requests are sending the path (`//v:h`) to `urlopen`: https://github.com/urllib3/urllib3/blob/d4ffa29ee1862b3d1afe584efb57d489a7659dac/src/urllib3/poolmanager.py#L444 https://github.com/psf/requests/blob/7a13c041dbef42f9f3feb14110f02626f6892e9a/src/requests/adapters.py#L487 and `urlopen` is now probably taking on too many responsibilities: https://github.com/urllib3/urllib3/blob/d4ffa29ee1862b3d1afe584efb57d489a7659dac/src/urllib3/connectionpool.py#L711-L712\nAlso, yes, I verified that RFC3986 allows `:` as a non-percent-encoded character in the path: https://datatracker.ietf.org/doc/html/rfc3986.html#section-3.3\nAnd I think the problem is the `//` in the path which is tripping up the parsing as `//` is the delimiter for what should be host and port after that. So in reality nothing is wrong here. I wonder if we need some kind of pre-parsing of the URL before sending it to `urlopen` to  trim this down. Something like `re.sub('^/+', '/')` would likely fix this in both Requests and urllib3.", "created_at": "2024-02-22T01:11:15Z"}
{"repo": "psf/requests", "pull_number": 6629, "instance_id": "psf__requests-6629", "issue_numbers": ["6628"], "base_commit": "7a13c041dbef42f9f3feb14110f02626f6892e9a", "patch": "diff --git a/src/requests/exceptions.py b/src/requests/exceptions.py\nindex e1cedf883d..83986b4898 100644\n--- a/src/requests/exceptions.py\n+++ b/src/requests/exceptions.py\n@@ -41,6 +41,16 @@ def __init__(self, *args, **kwargs):\n         CompatJSONDecodeError.__init__(self, *args)\n         InvalidJSONError.__init__(self, *self.args, **kwargs)\n \n+    def __reduce__(self):\n+        \"\"\"\n+        The __reduce__ method called when pickling the object must\n+        be the one from the JSONDecodeError (be it json/simplejson)\n+        as it expects all the arguments for instantiation, not just\n+        one like the IOError, and the MRO would by default call the\n+        __reduce__ method from the IOError due to the inheritance order.\n+        \"\"\"\n+        return CompatJSONDecodeError.__reduce__(self)\n+\n \n class HTTPError(RequestException):\n     \"\"\"An HTTP error occurred.\"\"\"\n", "test_patch": "diff --git a/tests/test_requests.py b/tests/test_requests.py\nindex 34796dc7ec..77aac3fecb 100644\n--- a/tests/test_requests.py\n+++ b/tests/test_requests.py\n@@ -2810,3 +2810,13 @@ def test_status_code_425(self):\n         assert r4 == 425\n         assert r5 == 425\n         assert r6 == 425\n+\n+\n+def test_json_decode_errors_are_serializable_deserializable():\n+    json_decode_error = requests.exceptions.JSONDecodeError(\n+        \"Extra data\",\n+        '{\"responseCode\":[\"706\"],\"data\":null}{\"responseCode\":[\"706\"],\"data\":null}',\n+        36,\n+    )\n+    deserialized_error = pickle.loads(pickle.dumps(json_decode_error))\n+    assert repr(json_decode_error) == repr(deserialized_error)\n", "problem_statement": "[BUG] JSONDecodeError can't be deserialized - invalid JSON raises a BrokenProcessPool and crashes the entire process pool\nHi all,\r\n\r\nI've stumbled upon a bug in the `requests` library, and have a proposal for a fix.\r\n\r\nIn short: I have a process pool running tasks in parallel, that are among other things doing queries to third-party APIs. One third-party returns an invalid JSON document as response in case of error.\r\n\r\nHowever, instead of just having a JSONDecodeError as the result of my job, the entire process pool crashes due to a BrokenProcessPool error with the following stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/concurrent/futures/process.py\", line 424, in wait_result_broken_or_wakeup\r\n    result_item = result_reader.recv()\r\n                  ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 251, in recv\r\n    return _ForkingPickler.loads(buf.getbuffer())\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/requests/exceptions.py\", line 41, in __init__\r\n    CompatJSONDecodeError.__init__(self, *args)\r\nTypeError: JSONDecodeError.__init__() missing 2 required positional arguments: 'doc' and 'pos'\r\n```\r\n\r\nSo I'm in a situation where receiving one invalid JSON as response is interrupting all my ongoing tasks because the entire process pool is crashing, and no more tasks can be submitted until the process pool recovered.\r\n\r\n## Origin of the bug + Fix\r\n\r\nAfter investigation, this is because the `requests.exception.JSONDecodeError` instances can't be deserialized once they've been serialized via `pickle`. So when the main process is trying to deserialize the error returned by the child process, the main process is crashing with to the error above.\r\n\r\nI think this bug has been around for a while, I've found old tickets from different projects mentioning issues that are looking similar: https://github.com/celery/celery/issues/5712\r\n\r\nI've pinpointed the bug to the following class: https://github.com/psf/requests/blob/main/src/requests/exceptions.py#L31\r\n\r\nBasically, due the MRO/order of inheritance, the `__reduce__` method used will not be the one of `CompatJSONDecodeError`. Most of the args will therefore be ditched when pickling the instance and it can't be deserialised back because `CompatJSONDecodeError.__init__`  does expect those args. MRO below:\r\n\r\n```\r\nIn [1]: from requests.exceptions import JSONDecodeError\r\n\r\nIn [2]: JSONDecodeError.__mro__\r\nOut[2]:\r\n(requests.exceptions.JSONDecodeError,\r\n requests.exceptions.InvalidJSONError,\r\n requests.exceptions.RequestException,\r\n OSError,\r\n simplejson.errors.JSONDecodeError,\r\n ValueError,\r\n Exception,\r\n BaseException,\r\n object)\r\n```\r\n\r\nI think the fix could be quite simple and should have very little side-effects: to specify a `JSONDecodeError.__reduce__` method that will call the one from the correct parent class (it will be regardless that it is json/simplejson via the Compat class, their respective methods having different signatures).\r\n\r\nI've taken the initiative to write a fix + a test and will raise a pull request to that effect \ud83d\ude4f \r\n\r\n-----\r\n\r\n## Expected Result\r\n\r\nI've written a test for this case: the error can easily be reproduced by simply trying to `pickle.dumps()`  then `pickle.loads()` on a error:\r\n\r\n```python\r\njson_decode_error = requests.exceptions.JSONDecodeError(\r\n    \"Extra data\",\r\n    '{\"responseCode\":[\"706\"],\"data\":null}{\"responseCode\":[\"706\"],\"data\":null}',\r\n    36,\r\n)\r\ndeserialized_error = pickle.loads(pickle.dumps(json_decode_error))\r\nassert repr(json_decode_error) == repr(deserialized_error)\r\n```\r\n\r\nThis assertion should be true\r\n\r\n## Actual Result\r\n\r\nCurrently, instead of passing it'll raise the following error:\r\n\r\n```\r\n>       CompatJSONDecodeError.__init__(self, *args)\r\nE       TypeError: JSONDecodeError.__init__() missing 2 required positional arguments: 'doc' and 'pos'\r\n```\r\n\r\n## Reproduction Steps\r\n\r\nAs mentioned above, this bug is more impactful in a multi-process architecture as it'll break the entire process pool.\r\nFor something looking a bit more like a live-case, I've produced a little snippet with a really simple API returning an invalid JSON: \r\n\r\n```python\r\n# File api.py\r\n\r\nfrom fastapi import FastAPI\r\nfrom starlette.responses import PlainTextResponse\r\n\r\napp = FastAPI()\r\n\r\n\r\n@app.get(\"/\")\r\nasync def root():\r\n    # An invalid json string returned by the endpoint that will trigger a JSONDecodeError when calling `res.json()`\r\n    s = '{\"responseCode\":[\"706\"],\"data\":null}{\"responseCode\":[\"706\"],\"data\":null}'\r\n    return PlainTextResponse(s, media_type=\"application/json\", status_code=400)\r\n\r\n\r\n# Run the API:\r\n# $ uvicorn api:app --reload\r\n#\r\n# curl http://127.0.0.1:8000 will return the invalid json\r\n```\r\n\r\nand the following \r\n\r\n```python\r\nfrom concurrent.futures import ProcessPoolExecutor\r\nfrom concurrent.futures.process import BrokenProcessPool\r\n\r\nimport requests\r\n\r\n\r\ndef my_task():\r\n    response = requests.get('http://127.0.0.1:8000/')\r\n    response.json()\r\n\r\ndef my_main_func():\r\n    with ProcessPoolExecutor(max_workers=4) as executor:\r\n        future = executor.submit(my_task)\r\n        for i in range(0, 5):\r\n            try:\r\n                future.result(timeout=100)\r\n                print(f\"Attempt {i} ok\")\r\n            except BrokenProcessPool:\r\n                print(f\"Attempt {i} - the pool is broken\")\r\n            except requests.JSONDecodeError:\r\n                print(f\"Attempt {i} raises a request JSONDecodeError\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    my_main_func()\r\n```\r\n\r\nInstead of getting the following output:\r\n```\r\nAttempt 0 raises a request JSONDecodeError\r\nAttempt 1 raises a request JSONDecodeError\r\nAttempt 2 raises a request JSONDecodeError\r\nAttempt 3 raises a request JSONDecodeError\r\nAttempt 4 raises a request JSONDecodeError\r\n```\r\n\r\nOne would currently have:\r\n```\r\nAttempt 0 - the pool is broken\r\nAttempt 1 - the pool is broken\r\nAttempt 2 - the pool is broken\r\nAttempt 3 - the pool is broken\r\nAttempt 4 - the pool is broken\r\n```\r\n\r\nAn invalid JSON is crashing the entire process pool and no job can be submitted anymore.\r\n\r\n## System Information\r\n\r\nTested with:\r\n- request==2.31.0\r\n- Python==3.11.7\r\n\r\n-----\r\n\r\nThanks a lot for taking the time to read this long bug report!!\n", "hints_text": "", "created_at": "2024-01-31T16:14:59Z"}
{"repo": "psf/requests", "pull_number": 6605, "instance_id": "psf__requests-6605", "issue_numbers": ["6604"], "base_commit": "3b5978fc5d1c121d70e83ddf59379cf36418b4ce", "patch": "diff --git a/docs/_templates/sidebarintro.html b/docs/_templates/sidebarintro.html\nindex 2b595b54d4..6a2fcea33e 100644\n--- a/docs/_templates/sidebarintro.html\n+++ b/docs/_templates/sidebarintro.html\n@@ -16,11 +16,11 @@\n \n <h3>Useful Links</h3>\n <ul>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/user/quickstart/\">Quickstart</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/user/advanced/\">Advanced Usage</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/api/\">API Reference</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/community/updates/#release-history\">Release History</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/dev/contributing/\">Contributors Guide</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/user/quickstart.html\">Quickstart</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/user/advanced.html\">Advanced Usage</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/api.html\">API Reference</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/community/updates.html#release-history\">Release History</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/dev/contributing.html\">Contributors Guide</a></li>\n \n   <p></p>\n \ndiff --git a/docs/_templates/sidebarlogo.html b/docs/_templates/sidebarlogo.html\nindex a3454b7c49..d0d14ce85b 100644\n--- a/docs/_templates/sidebarlogo.html\n+++ b/docs/_templates/sidebarlogo.html\n@@ -11,15 +11,15 @@\n \n <h3>Useful Links</h3>\n <ul>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/user/quickstart/\">Quickstart</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/user/advanced/\">Advanced Usage</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/api/\">API Reference</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/community/updates/#release-history\">Release History</a></li>\n-  <li><a href=\"https://requests.readthedocs.io/en/latest/dev/contributing/\">Contributors Guide</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/user/quickstart.html\">Quickstart</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/user/advanced.html\">Advanced Usage</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/api.html\">API Reference</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/community/updates.html#release-history\">Release History</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/dev/contributing.html\">Contributors Guide</a></li>\n \n   <p></p>\n \n-  <li><a href=\"https://requests.readthedocs.io/en/latest/community/recommended/\">Recommended Packages and Extensions</a></li>\n+  <li><a href=\"https://requests.readthedocs.io/en/latest/community/recommended.html\">Recommended Packages and Extensions</a></li>\n \n   <p></p>\n \n", "test_patch": "", "problem_statement": "Links at docs site return 404\nAt https://requests.readthedocs.io/en/latest/ all links under \"Usefull links\" lead to 404 page\r\n\r\n## Expected Result\r\n\r\nOpen corresponding docs page\r\n\r\n## Actual Result\r\n\r\n404 page\r\n\r\n## Reproduction Steps\r\n\r\n- Open https://requests.readthedocs.io/en/latest/\r\n- Click on \"[Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/)\"\r\n\r\n## System Information\r\nNot applicable\r\n\n", "hints_text": "Looks like `.html` at the end of link is what's missing\nWorking on it.", "created_at": "2023-12-17T21:36:12Z"}
{"repo": "psf/requests", "pull_number": 6600, "instance_id": "psf__requests-6600", "issue_numbers": ["5773", "5760"], "base_commit": "a25fde6989f8df5c3d823bc9f2e2fc24aa71f375", "patch": "diff --git a/docs/user/advanced.rst b/docs/user/advanced.rst\nindex c90a13dc6d..5e5e36f302 100644\n--- a/docs/user/advanced.rst\n+++ b/docs/user/advanced.rst\n@@ -1122,4 +1122,18 @@ coffee.\n \n     r = requests.get('https://github.com', timeout=None)\n \n+.. note:: The connect timeout applies to each connection attempt to an IP address.\n+          If multiple addresses exist for a domain name, the underlying ``urllib3`` will\n+          try each address sequentially until one successfully connects.\n+          This may lead to an effective total connection timeout *multiple* times longer\n+          than the specified time, e.g. an unresponsive server having both IPv4 and IPv6\n+          addresses will have its perceived timeout *doubled*, so take that into account\n+          when setting the connection timeout.\n+.. note:: Neither the connect nor read timeouts are `wall clock`_. This means\n+          that if you start a request, and look at the time, and then look at\n+          the time when the request finishes or times out, the real-world time\n+          may be greater than what you specified.\n+\n+\n+.. _`wall clock`: https://wiki.php.net/rfc/max_execution_wall_time\n .. _`connect()`: https://linux.die.net/man/2/connect\n", "test_patch": "", "problem_statement": "Connection timeout (not Read, Wall or Total) is consistently taking twice as long\nI'm aware that several issues related to timeout were opened (and closed) before, so I'm trying to narrow this report down to a very specific scope: **connection** timeout is behaving in a consistent, wrong way: it times out at _precisely_ **twice** the requested time.\r\n\r\nResults below are _so_ consistent we must acknowledge there is _something_ going on here! I **beg** you guys not to dismiss this report before taking a look at it!\r\n\r\nWhat this report is **not** about:\r\n- Total/Wall timeout:\r\nThat would be a nice feature, but I'm fully aware this is currently outside the scope of Requests. I'm focusing on _connection_ timeout only.\r\n\r\n- Read timeout:\r\nAll my tests were using http://google.com:81, which fails to even _connect_. There's no read involved, the server exists but never responds, not even to refuse the connection. No data is ever transmitted. No HTTP connection is ever established. This is **not** about `ReadTimeoutError`, this is about `ConnectTimeoutError`.\r\n\r\n- Accurate timings / network fluctuations:\r\nNot asking for millisecond precision. I don't even care about _whole seconds_ imprecision. But, surprisingly, `requests` is being incredibly accurate... to _twice_ the time.\r\n\r\n## Expected Result\r\n\r\n`requests.get('http://google.com:81', timeout=(4, 1))` should take _approximately_ 4 seconds to timeout.\r\n\r\n## Actual Result\r\n\r\nIt _consistently_ takes about 8.0 seconds to raise `requests.ConnectTimeout`. It always takes twice the time, for timeouts ranging from 1 to 100. Exception message clearly says in the end: `Connection to google.com timed out. (connect timeout=4)`, a very distinct message from read timeouts.\r\n\r\n\r\n## Reproduction Steps\r\n\r\n```python\r\nimport requests, time, os, sys\r\n\r\n# Using a know URL to test connection timeout\r\ndef test_timeout(timeout, url='http://google.com:81'):\r\n    start = time.time()\r\n    try:\r\n        requests.get(url, timeout=timeout)\r\n        print(\"OK!\")  # will never reach this...\r\n    except requests.ConnectTimeout:  # any other exception will bubble out\r\n        print('{}: {:.1f}'.format(timeout, time.time()-start))\r\n\r\nprint(\"\\n1 to 10, simple numeric timeout\")\r\nfor i in range(1, 11):\r\n    test_timeout(i)\r\n\r\nprint(\"\\n1 to 10, (x, 1) timeout tuple\")\r\nfor i in range(1, 11):\r\n    test_timeout((i, 1))\r\n\r\nprint(\"\\n1 to 10, (x, 10) timeout tuple\")\r\nfor i in range(1, 11):\r\n    test_timeout((i, 1))\r\n\r\nprint(\"\\nLarge timeouts\")\r\nfor i in (20, 30, 50, 100):\r\n    test_timeout((i, 1))\r\n```\r\n\r\nResults:\r\n```\r\nLinux desktop 5.4.0-66-generic #74~18.04.2-Ubuntu SMP Fri Feb 5 11:17:31 UTC 2021 x86_64\r\n3.6.9 (default, Jan 26 2021, 15:33:00) \r\n[GCC 8.4.0]\r\nRequests: 2.25.1\r\nUrllib3: 1.26.3\r\n\r\n1 to 10, simple numeric timeout\r\n1: 2.0\r\n2: 4.0\r\n3: 6.0\r\n4: 8.0\r\n5: 10.0\r\n6: 12.0\r\n7: 14.0\r\n8: 16.0\r\n9: 18.0\r\n10: 20.0\r\n\r\n1 to 10, (x, 1) timeout tuple\r\n(1, 1): 2.0\r\n(2, 1): 4.0\r\n(3, 1): 6.0\r\n(4, 1): 8.0\r\n(5, 1): 10.0\r\n(6, 1): 12.0\r\n(7, 1): 14.0\r\n(8, 1): 16.0\r\n(9, 1): 18.0\r\n(10, 1): 20.0\r\n\r\n1 to 10, (x, 10) timeout tuple\r\n(1, 10): 2.0\r\n(2, 10): 4.0\r\n(3, 10): 6.0\r\n(4, 10): 8.0\r\n(5, 10): 10.0\r\n(6, 10): 12.0\r\n(7, 10): 14.0\r\n(8, 10): 16.0\r\n(9, 10): 18.0\r\n(10, 10): 20.0\r\n\r\nLarge timeouts\r\n(20, 1): 40.0\r\n(30, 1): 60.0\r\n(50, 1): 100.1\r\n(100, 1): 200.2\r\n```\r\n\r\n## System Information\r\n```\r\n{\r\n  \"chardet\": {\r\n    \"version\": \"3.0.4\"\r\n  },\r\n  \"cryptography\": {\r\n    \"version\": \"3.2.1\"\r\n  },\r\n  \"idna\": {\r\n    \"version\": \"2.8\"\r\n  },\r\n  \"implementation\": {\r\n    \"name\": \"CPython\",\r\n    \"version\": \"3.6.9\"\r\n  },\r\n  \"platform\": {\r\n    \"release\": \"5.4.0-66-generic\",\r\n    \"system\": \"Linux\"\r\n  },\r\n  \"pyOpenSSL\": {\r\n    \"openssl_version\": \"1010108f\",\r\n    \"version\": \"17.5.0\"\r\n  },\r\n  \"requests\": {\r\n    \"version\": \"2.25.1\"\r\n  },\r\n  \"system_ssl\": {\r\n    \"version\": \"1010100f\"\r\n  },\r\n  \"urllib3\": {\r\n    \"version\": \"1.26.3\"\r\n  },\r\n  \"using_pyopenssl\": true\r\n}\r\n```\r\n\r\nIt seems there **is** a single, \"hidden\", connection retry, performed by either `requests` or `urllib3`, somewhere in the line. It has been reported by other users in other platforms too.\r\n\nrequests.get() timeout times out after twice the given value\nI made a simple example script to showcase what I experienced\r\n\r\n```python\r\nimport requests\r\nimport time\r\n\r\nprint('About to do a request that will timeout')\r\nstart_time = int(time.time())\r\ntry:\r\n    response = requests.get('http://www.google.com:81/', timeout=4)\r\nexcept requests.exceptions.RequestException as e:\r\n    print(f'Exception: {str(e)}')\r\nelapsed_secs = int(time.time()) - start_time\r\nprint(f'After request that should timeout. Elapsed seconds: {elapsed_secs}')\r\n```\r\n\r\n## Expected Result\r\n\r\nI expected this to timeout after 4 seconds and per the given timeout.\r\n\r\n## Actual Result\r\n\r\nWhat happens instead is that the timeout exception is raised after 2x the given timeout argument.\r\n\r\n```\r\nAbout to do a request that will timeout\r\nException: HTTPConnectionPool(host='www.google.com', port=81): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f204f680550>, 'Connection to www.google.com timed out. (connect timeout=4)'))\r\nAfter request that should timeout. Elapsed seconds: 8\r\n```\r\n\r\n## System Information\r\n\r\n    $ python -m requests.help\r\n\r\n```\r\n{\r\n  \"chardet\": {\r\n    \"version\": \"4.0.0\"\r\n  },\r\n  \"cryptography\": {\r\n    \"version\": \"\"\r\n  },\r\n  \"idna\": {\r\n    \"version\": \"2.10\"\r\n  },\r\n  \"implementation\": {\r\n    \"name\": \"CPython\",\r\n    \"version\": \"3.7.9\"\r\n  },\r\n  \"platform\": {\r\n    \"release\": \"5.10.16-arch1-1\",\r\n    \"system\": \"Linux\"\r\n  },\r\n  \"pyOpenSSL\": {\r\n    \"openssl_version\": \"\",\r\n    \"version\": null\r\n  },\r\n  \"requests\": {\r\n    \"version\": \"2.25.1\"\r\n  },\r\n  \"system_ssl\": {\r\n    \"version\": \"101010af\"\r\n  },\r\n  \"urllib3\": {\r\n    \"version\": \"1.26.3\"\r\n  },\r\n  \"using_pyopenssl\": false\r\n}\r\n\r\n```\r\n\n", "hints_text": "When using `urllib3` directly, it seems to also give a very consistent timeout of **8 times** the requested one:\r\n\r\n```pycon\r\n>>> import urllib3, time\r\n>>> http = urllib3.PoolManager()\r\n>>> try:\r\n...     start = time.time()\r\n...     http.request('GET', 'http://google.com:81', timeout=1)\r\n... except urllib3.exceptions.MaxRetryError:\r\n...     print(time.time() - start)\r\n... \r\n8.030268907546997\r\n>>> try:\r\n...     start = time.time()\r\n...     http.request('GET', 'http://google.com:81', timeout=2)\r\n... except urllib3.exceptions.MaxRetryError:\r\n...     print(time.time() - start)\r\n... \r\n16.021981477737427\r\n>>> try:\r\n...     start = time.time()\r\n...     http.request('GET', 'http://google.com:81', timeout=3)\r\n... except urllib3.exceptions.MaxRetryError:\r\n...     print(time.time() - start)\r\n... \r\n24.041129112243652\r\n```\nUrllib3's [documentation on retrying requests](https://urllib3.readthedocs.io/en/latest/user-guide.html#retrying-requests) say:\r\n\r\n> urllib3 can automatically retry idempotent requests. This same mechanism also handles redirects. You can control the retries using the retries parameter to request(). By default, urllib3 will retry requests 3 times and follow up to 3 redirects.\r\n\r\n3 retries = 4 connection attempts, taking that into account still leads us to the same issue: _each attempt is taking twice as long to timeout_\nAnd, regarding the last comment on duplicate issue #5760 by @sigmavirus24:\r\n\r\n> You're asking for a wall clock timeout and we can not provide that.\r\n\r\nI'm **not** asking for a wall clock timeout, nor a total request timeout. I'm asking for a fix in _Connection_ timeout. That double time is a bug.\r\n\r\n> Further we have clearly documented how timeouts work in our docs\r\n\r\nTrue, and my tests show that connection timeouts are not behaving as the documentation says it should. Where in the documentation this double time behavior is explained?\nI think I've found the culprit: **IPv6**! It seems requests/urllib3 is automatically trying to connect using both IPv4 and IPv6, and that accounts for the doubled time.\r\n\r\nI'll do some more tests to properly isolate the problem, as it seems requests is trying IPv6 _even when it's not available_, raising a `ConnectionError` with `Failed to establish a new connection: [Errno 101] Network is unreachable`, which is undesirable.\nhttps://github.com/urllib3/urllib3/issues/2030\n> [urllib3/urllib3#2030](https://github.com/urllib3/urllib3/issues/2030)\r\n\r\nI'm not sure that issue is related to this one. I'm not experiencing connections that are slower/faster depending on IP family, I'm experiencing an exact _double_ timeout time due to (trying to) connect using both IPv4 and IPv6, and failing in both families.\r\n\r\nIs this \"if IPv4 fails, retry using IPv6\" a known/expected behavior? Is this documented anywhere?\nAfter more tests, the issue is really the dual IPv4/IPv6 connection attempts. Using the workaround proposed at  by a [Stackoverflow answer](https://stackoverflow.com/a/46972341/624066) to force either IPv4 or IPv6 *only*, timeout behaves as expected:\r\n\r\n```python\r\n# Monkey-patch urllib3 to force IPv4-connections only.\r\n# Adapted from https://stackoverflow.com/a/46972341/624066\r\nimport socket\r\nimport urllib3.util.connection\r\ndef allowed_gai_family():\r\n\treturn socket.AF_INET\r\n\r\nimport urllib3\r\nimport requests\r\nimport time, os, sys\r\n\r\n# Using a know URL to test connection timeout\r\nURL='http://google.com:81'\r\n\r\nhttp = urllib3.PoolManager()\r\n\r\ndef test_urllib3_timeout(timeout, url=URL):\r\n    start = time.time()\r\n    try:\r\n        http.request('GET', url, timeout=timeout, retries=0)\r\n        print(\"OK!\")\r\n    except urllib3.exceptions.MaxRetryError:\r\n        print('{}: {:.1f}'.format(timeout, time.time()-start))\r\n\r\ndef test_requests_timeout(timeout, url=URL):\r\n    start = time.time()\r\n    try:\r\n        requests.get(url, timeout=timeout)\r\n        print(\"OK!\")  # will never reach this...\r\n    except requests.ConnectTimeout:  # any other exception will bubble out\r\n        print('{}: {:.1f}'.format(timeout, time.time()-start))\r\n\r\ndef test_timeouts():\r\n    print(\"\\nUrllib3\")\r\n    for i in range(1, 6):\r\n        test_urllib3_timeout(i)\r\n\r\n    print(\"\\nRequests\")\r\n    for i in range(1, 6):\r\n        test_requests_timeout((i, 1))\r\n\r\n\r\nprint(\"BEFORE PATCH:\")\r\ntest_timeouts()\r\n\r\nurllib3.util.connection.allowed_gai_family = allowed_gai_family\r\n\r\nprint(\"\\nAFTER PATCH:\")\r\ntest_timeouts()\r\n```\r\nResults:\r\n```\r\nBEFORE PATCH:\r\n\r\nUrllib3\r\n1: 2.0\r\n2: 4.0\r\n3: 6.0\r\n4: 8.0\r\n5: 10.0\r\n\r\nRequests\r\n(1, 1): 2.0\r\n(2, 1): 4.0\r\n(3, 1): 6.0\r\n(4, 1): 8.0\r\n(5, 1): 10.0\r\n\r\nAFTER PATCH:\r\n\r\nUrllib3\r\n1: 1.0\r\n2: 2.0\r\n3: 3.0\r\n4: 4.0\r\n5: 5.0\r\n\r\nRequests\r\n(1, 1): 1.0\r\n(2, 1): 2.0\r\n(3, 1): 3.0\r\n(4, 1): 4.0\r\n(5, 1): 5.0\r\n\nStill, I believe at least `requests` documentation should mention this behavior. for example, https://requests.readthedocs.io/en/latest/user/advanced/#timeouts could add something like this:\r\n\r\n> When the client has IPv6 support and the server has an IPv6 DNS record (AAAA), if the IPv4 connection fails the underlying `urllib3` will automatically retry using IPv6, which may lead to an effective connection timeout of twice the specified time, so take that into account when setting the connection timeout.\n@MestreLion I would accept a PR adding an explanation of what can go wrong with timeouts like this\nJust popping in here to say thanks for digging into this. This would explain my double timeout question on the other issue I opened too.\r\n\r\nI just tested by forcing IPv4 and the timeout is no longer doubled\nexplanation of what can be  done fixe the  timeouts \nIn the future, please search **closed and** open issues before creating new ones that are duplicates.\nHello @sigmavirus24 thank you for your response.\r\n\r\nI did do that but could not find one. I can of course only judge by the issue's title and what github search can afford to show me.\r\n\r\n Can you please at least link the issue for which you are closing this one as a duplicate? Thanks in advance.\nOne such issue I could find is #5450, the issue was closed but not solved.\r\n\r\nI'm also experiencing this problem, and requests is _consistently_ timing out after approximately *twice* the specified timeout. This is **not** a precision problem, this is an actual bug!\r\n\r\nPlease reopen either this or the other issue!\r\n\r\n```pycon\r\nPython 3.6.9 (default, Jan 26 2021, 15:33:00) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import time, requests\r\n>>> requests.__version__\r\n'2.25.1'\r\n>>> requests.urllib3.__version__\r\n'1.26.3'\r\n>>> def test_timeout(timeout=1, url='http://google.com:81'):\r\n...     start = time.time()\r\n...     try:\r\n...             requests.get(url, timeout=timeout)\r\n...     except requests.ConnectTimeout:\r\n...             print(time.time()-start)\r\n... \r\n>>> for i in range(1, 11):\r\n...     test_timeout(i)\r\n... \r\n2.0313971042633057\r\n4.0080249309539795\r\n6.010318040847778\r\n8.012223482131958\r\n10.014029264450073\r\n12.012367725372314\r\n14.017669200897217\r\n16.01678204536438\r\n18.01997184753418\r\n20.043574571609497\r\n>>> \r\n```\r\n\r\nThis is a `ConnectTimeout`, no Read Timeout involved. Look at the timings, it can't be more consistent than this!\n#4276 #3099\r\n\r\nYou're asking for a wall clock timeout and we can not provide that. Further we have clearly documented how timeouts work in our docs", "created_at": "2023-12-13T13:19:43Z"}
